#!/usr/bin/env bash
U="${SE_LIB_URL:-https://public.megabyte.space/source.sh}"
P="${SE_LIB_PATH:-}"
if [[ -n "$P" && -r "$P" ]]; then . "$P"; else
  C=(/usr/local/lib/shell-expert/source.sh ./source.sh "${XDG_CACHE_HOME:-$HOME/.cache}/shell-expert/source.sh")
  for p in "${C[@]}"; do [[ -r "$p" ]] && { . "$p"; P="$p"; break; }; done
  if [[ -z "$P" ]]; then
    D="${XDG_CACHE_HOME:-$HOME/.cache}/shell-expert"; mkdir -p "$D" || { printf 'ERR: cache\n' >&2; exit 74; }
    T="$(mktemp "$D/.dl.XXXX")" || { printf 'ERR: mktemp\n' >&2; exit 70; }
    if command -v curl >/dev/null; then curl -fsS --connect-timeout 5 -m 20 --retry 3 --retry-all-errors -o "$T" "$U" || { printf 'ERR: fetch\n' >&2; exit 69; }
    elif command -v wget >/dev/null; then wget -q -O "$T" "$U" || { printf 'ERR: fetch\n' >&2; exit 69; }
    else printf 'ERR: need curl/wget\n' >&2; exit 69; fi
    bash -n "$T" || { printf 'ERR: syntax\n' >&2; exit 70; }
    P="$D/source.sh"; mv -f "$T" "$P"; chmod 0644 "$P" || true
    [[ -d /usr/local/lib && -w /usr/local/lib ]] && { mkdir -p /usr/local/lib/shell-expert 2>/dev/null || true; cp -f "$P" /usr/local/lib/shell-expert/source.sh 2>/dev/null || true; }
    . "$P"
  fi
fi
VERSION="${VERSION:-$(TZ=America/New_York date +'%Y%m%d-%H%M%S')}"

UPDATE_URL="https://example.com/pbs-setup.sh"  # set to your canonical URL if you host this
SLUG="$(se_slug_from_url "$UPDATE_URL")"
se_init_context "$SLUG" "$UPDATE_URL"; se_set_update_url "$SLUG" "$UPDATE_URL"
se_self_update "$SLUG" "$UPDATE_URL" "/usr/local/bin/$SLUG"

set -o pipefail

# =========================
# Defaults (override via env)
# =========================
PBS_LOCAL_NAME="${PBS_LOCAL_NAME:-pbs-local}"
R2_DS_NAME="${R2_DS_NAME:-cloudflare-r2}"
WASABI_DS_NAME="${WASABI_DS_NAME:-wasabi}"

# Retention you wanted
RET_LOCAL_DAILY="${RET_LOCAL_DAILY:-7}"
RET_LOCAL_WEEKLY="${RET_LOCAL_WEEKLY:-4}"
RET_LOCAL_MONTHLY="${RET_LOCAL_MONTHLY:-6}"

RET_S3_DAILY="${RET_S3_DAILY:-7}"
RET_S3_WEEKLY="${RET_S3_WEEKLY:-4}"

# Schedules
PRUNE_SCHED="${PRUNE_SCHED:-*-*-* 03:00:00}"      # prune job schedule
SYNC_SCHED="${SYNC_SCHED:-*-*-* 04:30:00}"        # push sync job schedule
GC_SCHED="${GC_SCHED:-Sun *-*-* 05:10:00}"        # datastore GC schedule (>24h after prune)  :contentReference[oaicite:1]{index=1}

# Self remote (loopback)
SELF_REMOTE_NAME="${SELF_REMOTE_NAME:-self}"
SELF_REMOTE_AUTH_ID="${SELF_REMOTE_AUTH_ID:-root@pam}"
SELF_REMOTE_HOST="${SELF_REMOTE_HOST:-127.0.0.1}"
SELF_REMOTE_PORT="${SELF_REMOTE_PORT:-8007}"
SELF_REMOTE_PASSWORD="${SELF_REMOTE_PASSWORD:-}"

# Behavior flags
RUN_INITIAL_SYNC="${RUN_INITIAL_SYNC:-0}"
STRICT_DATASTORE_CHECK="${STRICT_DATASTORE_CHECK:-1}"
DRY_RUN="${DRY_RUN:-0}"
CONFIRM_COMMAND="${CONFIRM_COMMAND:-0}"

# =========================
# Helpers
# =========================
require_cmds=(proxmox-backup-manager jq)

prompt_if_empty() {
  local var="$1" prompt="$2" secret="${3:-0}" val
  val="${!var}"
  if [[ -z "$val" ]]; then
    if [[ "$secret" == "1" ]]; then read -r -s -p "$prompt: " val; printf '\n'
    else read -r -p "$prompt: " val; fi
    printf -v "$var" '%s' "$val"; export "$var"
  fi
}

_run() {
  if [[ "$DRY_RUN" == "1" ]]; then
    se_log CMD "(dry-run) $*"
    return 0
  fi
  se_cmd "$@"
}

fail() { se_log FATAL "$*"; return 70; }

pbs_major_version() { proxmox-backup-manager version 2>/dev/null | awk '{print $2}' | cut -d. -f1; }

datastore_exists() { proxmox-backup-manager datastore show "$1" >/dev/null 2>&1; }

# =========================
# Steps
# =========================
step_10_install() {
  se_log INFO "Ensuring dependencies present"
  se_ensure_cmds "${require_cmds[@]}" || return 69

  local v; v="$(pbs_major_version || true)"
  if [[ -z "$v" ]]; then
    se_log WARN "Could not determine PBS version; continuing"
  elif (( v < 4 )); then
    fail "PBS $v detected; this script expects PBS ≥ 4 (prune jobs + GC schedules)."
  fi

  printf '%s\n' "{\"installed_at\":\"$(TZ=America/New_York date +'%F %T')\",\"update_url\":\"$UPDATE_URL\",\"version\":\"$VERSION\"}" \
    | se_config_write "$SLUG" || return 78
}

step_20_collect_config() {
  se_log INFO "Collecting credentials for remote '${SELF_REMOTE_NAME}' (self push)"
  prompt_if_empty SELF_REMOTE_PASSWORD "PBS password/API token for ${SELF_REMOTE_AUTH_ID}@${SELF_REMOTE_HOST}:${SELF_REMOTE_PORT}" 1
}

step_30_verify_datastores() {
  se_log INFO "Verifying that required datastores exist (no creation is performed)"
  local missing=0
  for ds in "$PBS_LOCAL_NAME" "$R2_DS_NAME" "$WASABI_DS_NAME"; do
    if datastore_exists "$ds"; then
      se_log INFO "Found datastore: $ds"
    else
      se_log ${STRICT_DATASTORE_CHECK:+ERROR} ${STRICT_DATASTORE_CHECK:+"Datastore missing: $ds (STRICT_DATASTORE_CHECK=1)"}
      ((missing++))
    fi
  done
  if (( missing > 0 )) && [[ "$STRICT_DATASTORE_CHECK" == "1" ]]; then
    fail "One or more datastores are missing; create them first and re-run."
  fi
}

# ---- GC: set schedule on datastore (NOT prune) ----
_update_gc_schedule() {
  local ds="$1"
  if datastore_exists "$ds"; then
    _run proxmox-backup-manager datastore update "$ds" --gc-schedule "$GC_SCHED" || return 70
  else
    se_log WARN "Skipping GC schedule (datastore not found): $ds"
  fi
}

# ---- Prune Jobs: create/update per datastore with keep-* retention ----
_create_or_update_prune_job() {
  local job_id="$1" store="$2" keep_args=("${@:3}")
  if ! datastore_exists "$store"; then
    se_log WARN "Skipping prune job '${job_id}' (datastore not found): ${store}"
    return 0
  fi
  if proxmox-backup-manager prune-job show "$job_id" >/dev/null 2>&1; then
    _run proxmox-backup-manager prune-job update "$job_id" \
      --store "$store" \
      --schedule "$PRUNE_SCHED" \
      "${keep_args[@]}" || return 70
  else
    _run proxmox-backup-manager prune-job create "$job_id" \
      --store "$store" \
      --schedule "$PRUNE_SCHED" \
      "${keep_args[@]}" || return 70
  fi
}

step_40_configure_prune_and_gc() {
  se_log INFO "Configuring prune jobs (retention) and GC schedules"
  # Local retention: 7 daily, 4 weekly, 6 monthly
  _create_or_update_prune_job "prune-${PBS_LOCAL_NAME}" "$PBS_LOCAL_NAME" \
    --keep-daily "$RET_LOCAL_DAILY" \
    --keep-weekly "$RET_LOCAL_WEEKLY" \
    --keep-monthly "$RET_LOCAL_MONTHLY" || return 70

  # Cloud S3 retention: 7 daily, 4 weekly
  _create_or_update_prune_job "prune-${R2_DS_NAME}" "$R2_DS_NAME" \
    --keep-daily "$RET_S3_DAILY" \
    --keep-weekly "$RET_S3_WEEKLY" || return 70

  _create_or_update_prune_job "prune-${WASABI_DS_NAME}" "$WASABI_DS_NAME" \
    --keep-daily "$RET_S3_DAILY" \
    --keep-weekly "$RET_S3_WEEKLY" || return 70

  # GC schedules on each datastore
  _update_gc_schedule "$PBS_LOCAL_NAME" || return 70
  _update_gc_schedule "$R2_DS_NAME" || return 70
  _update_gc_schedule "$WASABI_DS_NAME" || return 70
}

# ---- Remote (loopback) for push sync ----
step_50_create_or_update_self_remote() {
  se_log INFO "Ensuring remote '${SELF_REMOTE_NAME}' -> ${SELF_REMOTE_HOST}:${SELF_REMOTE_PORT}"
  if proxmox-backup-manager remote list | grep -q -E "^$SELF_REMOTE_NAME\$"; then
    _run proxmox-backup-manager remote update "$SELF_REMOTE_NAME" \
      --host "$SELF_REMOTE_HOST" --port "$SELF_REMOTE_PORT" \
      --auth-id "$SELF_REMOTE_AUTH_ID" --password "$SELF_REMOTE_PASSWORD" || return 70
  else
    _run proxmox-backup-manager remote create "$SELF_REMOTE_NAME" \
      --host "$SELF_REMOTE_HOST" --port "$SELF_REMOTE_PORT" \
      --auth-id "$SELF_REMOTE_AUTH_ID" --password "$SELF_REMOTE_PASSWORD" || return 70
  fi
}

# ---- Push sync jobs ----
_create_or_update_push_sync() {
  local job_id="$1" target_ds="$2"
  if ! datastore_exists "$target_ds"; then
    se_log WARN "Skipping sync-job '${job_id}' (target datastore not found): ${target_ds}"
    return 0
  fi
  se_log INFO "Ensuring push sync job '${job_id}' : ${PBS_LOCAL_NAME} -> ${target_ds}"
  if proxmox-backup-manager sync-job show "$job_id" >/dev/null 2>&1; then
    _run proxmox-backup-manager sync-job update "$job_id" \
      --remote "$SELF_REMOTE_NAME" \
      --remote-store "$PBS_LOCAL_NAME" \
      --store "$target_ds" \
      --sync-direction push \
      --schedule "$SYNC_SCHED" \
      --remove-vanished 0 || return 70
  else
    _run proxmox-backup-manager sync-job create "$job_id" \
      --remote "$SELF_REMOTE_NAME" \
      --remote-store "$PBS_LOCAL_NAME" \
      --store "$target_ds" \
      --sync-direction push \
      --schedule "$SYNC_SCHED" \
      --remove-vanished 0 || return 70
  fi
}

step_60_sync_jobs() {
  _create_or_update_push_sync "push-to-r2" "$R2_DS_NAME" || return 70
  _create_or_update_push_sync "push-to-wasabi" "$WASABI_DS_NAME" || return 70
}

step_70_initial_sync_optional() {
  if [[ "$RUN_INITIAL_SYNC" == "1" ]]; then
    se_log INFO "Running initial sync jobs now…"
    _run proxmox-backup-manager sync-job run push-to-r2 || return 70
    _run proxmox-backup-manager sync-job run push-to-wasabi || return 70
  else
    se_log INFO "Initial sync skipped (set RUN_INITIAL_SYNC=1 to run immediately)"
  fi
}

step_80_summary() {
  se_log INFO "Summary: datastores"
  proxmox-backup-manager datastore list --output-format json-pretty || true
  se_log INFO "Summary: prune-jobs"
  proxmox-backup-manager prune-job list --output-format json-pretty || true
  se_log INFO "Summary: sync-jobs"
  proxmox-backup-manager sync-job list --output-format json-pretty || true
  se_log INFO "Done. Retention via prune-jobs; GC via datastore schedule."
}

# =========================
# Harness
# =========================
main() {
  local hc; hc="$(se__hc_build_url "$SLUG")"; se_hc_start "$hc"
  se_ndjson_init "$SLUG"
  if ! se_run_steps \
      step_10_install \
      step_20_collect_config \
      step_30_verify_datastores \
      step_40_configure_prune_and_gc \
      step_50_create_or_update_self_remote \
      step_60_sync_jobs \
      step_70_initial_sync_optional \
      step_80_summary; then
    local rc=$?; se_ndjson_finalize "$VERSION" "$rc" "${START_FROM:-}" "" "fail"; se_hc_fail "$hc"; return "$rc"
  fi
  se_ndjson_finalize "$VERSION" 0 "${START_FROM:-}" "" "ok"; se_hc_success "$hc"; return 0
}

case "${1:-run}" in
  install)
    printf '%s\n' "{\"installed_at\":\"$(TZ=America/New_York date +'%F %T')\",\"update_url\":\"$UPDATE_URL\",\"version\":\"$VERSION\"}" \
      | se_config_write "$SLUG"
    ;;
  debug) DEBUG=1 CONFIRM_COMMAND=${CONFIRM_COMMAND:-1} main "$@";;
  uninstall)
    rm -rf "$(se_xdg_config_dir "$SLUG")" "$(se_xdg_state_dir "$SLUG")" "$(se_xdg_cache_dir "$SLUG")"
    ;;
  self-update) se_self_update "$SLUG" "$UPDATE_URL" "/usr/local/bin/$SLUG";;
  recover) se_recover_shell "$SLUG";;
  help)
    cat <<'EOF'
Proxmox Backup Server — prune jobs + GC schedule + push sync (existing datastores)
-----------------------------------------------------------------------------------
What this does (idempotent):
  • DOES NOT create datastores.
  • Verifies these exist: pbs-local, cloudflare-r2, wasabi.
  • Creates/updates PRUNE JOBS (retention & schedule):
      - pbs-local: keep-daily=7, keep-weekly=4, keep-monthly=6; schedule 03:00
      - cloudflare-r2: keep-daily=7, keep-weekly=4;               schedule 03:00
      - wasabi:        keep-daily=7, keep-weekly=4;               schedule 03:00
  • Sets GC SCHEDULE on each datastore (NOT via prune job): Sunday 05:10 (24h+ after prune).
  • Creates/updates PUSH SYNC JOBS (schedule 04:30):
      - pbs-local -> cloudflare-r2 (push-to-r2)
      - pbs-local -> wasabi        (push-to-wasabi)

Why this change?
  PBS ≥4 moved retention to “prune jobs”. Datastore “keep-*” flags now error out.
  GC remains a datastore-level schedule. See: PBS docs (Prune Jobs; GC scheduling).  [docs]

Schedules (default)
  PRUNE_SCHED   "*-*-* 03:00:00"
  SYNC_SCHED    "*-*-* 04:30:00"
  GC_SCHED      "Sun *-*-* 05:10:00"

Environment variables
  PBS_LOCAL_NAME, R2_DS_NAME, WASABI_DS_NAME
  RET_LOCAL_DAILY/RET_LOCAL_WEEKLY/RET_LOCAL_MONTHLY
  RET_S3_DAILY/RET_S3_WEEKLY
  PRUNE_SCHED, SYNC_SCHED, GC_SCHED
  SELF_REMOTE_NAME/SELF_REMOTE_AUTH_ID/SELF_REMOTE_HOST/SELF_REMOTE_PORT/SELF_REMOTE_PASSWORD
  STRICT_DATASTORE_CHECK(=1), RUN_INITIAL_SYNC(=0), DRY_RUN(=0), CONFIRM_COMMAND(=0)

Usage
  sudo bash pbs-setup.sh install
  sudo bash pbs-setup.sh run
  sudo bash pbs-setup.sh debug
  sudo bash pbs-setup.sh self-update
  sudo bash pbs-setup.sh uninstall

Notes
  - Keep prune → wait ≥24h → GC, per PBS atime/grace period guidance.
  - You can change hours to fit your backup window; keep ordering: backup → prune → sync → GC.

[docs] Prune Jobs & GC schedule are documented in PBS “Maintenance Tasks”.
EOF
    ;;
  run|*) main "$@";;
esac
exit $?
