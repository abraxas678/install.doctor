#!/usr/bin/env bash
# ==============================================================================
# restore-rpool-data.sh — Safe ZFS restore of *everything but the Proxmox OS*
#
# Restores a ZFS backup (FULL + INCREMENTAL .zst streams) into a *temporary*
# namespace under the existing 'rpool' (fresh Proxmox install), then replicates
# all datasets EXCEPT 'rpool/ROOT/*' back to their canonical locations under
# 'rpool', preserving snapshot history — without touching your clean boot/OS.
#
# Features
# • Prompts or ENV inputs (FULL_ZST, INCR_ZST) with smart defaults:
#     FULL_ZST defaults to ./rpool.full.zst
#     INCR_ZST defaults to newest ./*.incr.zst if present (or blank to skip)
# • Progress meters with pv (if installed), graceful fallback otherwise
# • Idempotent-ish: receives into rpool/RESTORE, then replicates per-top dataset
# • Preserves snapshot history (oldest → newest) when replicating to rpool/*
# • Never overwrites rpool/ROOT (your fresh Proxmox OS/boot)
#
# Usage
#   FULL_ZST=/path/to/rpool.full.zst INCR_ZST=/path/to/20250831-175643.incr.zst \
#     ./restore-rpool-data.sh
#
# Or run with no env vars and use the interactive prompts.
#
# Requirements
#   - Run as root on the freshly installed Proxmox host
#   - zfs, zpool, zstd installed (pv optional)
#
# Safety Notes
#   - We *receive* into rpool/RESTORE first. Your fresh rpool/ROOT stays intact.
#   - We then replicate all top-level children under rpool/RESTORE/* (except ROOT)
#     back to rpool/*, using -R to include descendants & snapshots.
#   - Existing targets under rpool/* will be force-received (-F). If they contain
#     data you care about, back them up first.
#
# Design
#   - Clear function boundaries, strict mode, traps, explicit checks
#   - Avoids creating extra temporary pools/devices
#   - Works even if you only have a single disk hosting 'rpool'
# ==============================================================================

set -Eeuo pipefail

# ------------------------------ Config Defaults --------------------------------
RESTORE_NS="rpool/RESTORE"   # Temporary receive namespace inside rpool
POOL_NAME="rpool"            # The pool to restore *into* and final target
PV_BIN="${PV_BIN:-pv}"       # Override if pv is elsewhere

# ------------------------------ UI Helpers -------------------------------------
log()  { printf "\033[1;34m[%s]\033[0m %s\n" "INFO" "$*"; }
warn() { printf "\033[1;33m[%s]\033[0m %s\n" "WARN" "$*"; }
err()  { printf "\033[1;31m[%s]\033[0m %s\n" "ERR " "$*" >&2; }
die()  { err "$*"; exit 1; }

need_bin() {
  command -v "$1" >/dev/null 2>&1 || die "Required binary not found: $1"
}

confirm() {
  local prompt="${1:-Are you sure? [y/N]}"; local reply
  read -r -p "$prompt " reply || true
  [[ "${reply,,}" == "y" || "${reply,,}" == "yes" ]]
}

# ------------------------------ Preconditions ----------------------------------
trap 'err "Script aborted (exit code $?)."; exit 1' ERR

need_bin zfs
need_bin zpool
need_bin zstd
if ! command -v "$PV_BIN" >/dev/null 2>&1; then
  warn "pv not found — progress bars will be limited. Install with: apt install pv"
  PV_BIN=""
fi

# Ensure we're root
[[ "${EUID:-$(id -u)}" -eq 0 ]] || die "Please run as root."

# Ensure the target pool exists and is imported
if ! zpool list -H -o name | grep -qx "$POOL_NAME"; then
  die "ZFS pool '$POOL_NAME' is not imported. Import or install Proxmox with ZFS first."
fi

# Ensure the fresh boot dataset exists (we won't touch it)
if ! zfs list -H -o name "${POOL_NAME}/ROOT" >/dev/null 2>&1; then
  die "Expected dataset '${POOL_NAME}/ROOT' missing. Are you on a fresh ZFS-root Proxmox?"
fi

# ------------------------------ Inputs & Defaults ------------------------------
default_full="./rpool.full.zst"
default_incr="$(ls -1t ./*.incr.zst 2>/dev/null | head -n1 || true)"

FULL_ZST="${FULL_ZST:-}"
INCR_ZST="${INCR_ZST:-}"

prompt_file() {
  local varname="$1" default="$2" label="$3"
  local current="${!varname:-}"
  if [[ -z "$current" ]]; then
    local prompt="Path for ${label}"
    [[ -n "$default" ]] && prompt+=" [${default}]"
    prompt+=": "
    read -r -p "$prompt" current || true
    current="${current:-$default}"
    printf -v "$varname" "%s" "$current"
  fi
}

log "Collecting input…"
prompt_file FULL_ZST "$default_full" "FULL backup (.zst) — e.g., rpool.full.zst"
if [[ -z "$INCR_ZST" && -n "$default_incr" ]]; then
  warn "Auto-detected newest incremental: $default_incr"
fi
prompt_file INCR_ZST "$default_incr" "INCREMENTAL backup (.zst) — e.g., 20250831-175643.incr.zst (leave blank to skip)"

[[ -f "$FULL_ZST" ]] || die "FULL backup not found: $FULL_ZST"
if [[ -n "$INCR_ZST" && ! -f "$INCR_ZST" ]]; then
  die "Specified INCREMENTAL not found: $INCR_ZST"
fi

log "Full  : $FULL_ZST"
log "Incr  : ${INCR_ZST:-<none>}"
log "Pool  : $POOL_NAME"
log "Temp  : $RESTORE_NS"

# ------------------------------ Core Functions ---------------------------------

ensure_namespace() {
  if zfs list -H -o name "$RESTORE_NS" >/dev/null 2>&1; then
    warn "Namespace '$RESTORE_NS' already exists."
    if confirm "OK to re-use and overwrite content within '$RESTORE_NS'? [y/N]"; then
      return 0
    else
      die "User aborted."
    fi
  else
    log "Creating namespace '$RESTORE_NS'…"
    zfs create -p "$RESTORE_NS"
  fi
}

stream_into_namespace() {
  local zst_file="$1"
  local label="$2"  # FULL or INCR

  log "Receiving $label stream into '$RESTORE_NS'…"
  # We receive at the namespace root; ZFS will create children accordingly.
  # Use pv if available for progress.
  if [[ -n "$PV_BIN" ]]; then
    # Try to estimate size for pv: falls back if du fails (e.g., remote fs)
    local size_bytes
    size_bytes="$(du -b "$zst_file" 2>/dev/null | awk '{print $1}')" || size_bytes=""
    if [[ -n "$size_bytes" ]]; then
      zstd -dc -- "$zst_file" | "$PV_BIN" -s "$size_bytes" | zfs recv -F "$RESTORE_NS"
    else
      zstd -dc -- "$zst_file" | "$PV_BIN" | zfs recv -F "$RESTORE_NS"
    fi
  else
    zstd -dc -- "$zst_file" | zfs recv -F "$RESTORE_NS"
  fi
}

# Return a space-separated list of top-level children under RESTORE_NS, excluding ROOT
list_toplevel_children() {
  zfs list -H -o name "$RESTORE_NS"/* 2>/dev/null | grep -vE "^${RESTORE_NS}/ROOT($|/)" || true
}

# For a dataset, find oldest and newest snapshots (names without pool prefix)
# Prints: "<oldest>@snap  <newest>@snap" (full names)
dataset_oldest_newest_snaps() {
  local ds="$1"
  # List snapshots under this dataset (recursive) but we want *this dataset's* own snapshots
  local snaps
  snaps="$(zfs list -H -t snapshot -o name -S creation -r "$ds" | awk -F@ -v pfx="$ds" '$1==pfx{print $0}')" || true
  [[ -n "$snaps" ]] || return 1
  local newest oldest
  newest="$(echo "$snaps" | head -n1)"
  oldest="$(echo "$snaps" | tail -n1)"
  # Note: -S creation sorts newest first, hence tail=oldest
  printf "%s %s\n" "$oldest" "$newest"
}

replicate_dataset_to_rpool() {
  local src="$1"                          # e.g., rpool/RESTORE/data
  local rel="${src#${RESTORE_NS}/}"       # e.g., data
  local dst="${POOL_NAME}/${rel}"         # e.g., rpool/data

  log "Preparing to replicate: $src  →  $dst"

  # Ensure target parent exists
  local parent
  parent="$(dirname "$dst")"
  if ! zfs list -H -o name "$parent" >/dev/null 2>&1; then
    log "Creating parent dataset: $parent"
    zfs create -p "$parent"
  fi

  # Find oldest/newest snapshots on the *source* dataset
  local line
  if ! line="$(dataset_oldest_newest_snaps "$src")"; then
    warn "No snapshots found for $src — creating a temporary snapshot to send current state."
    local ts
    ts="$(date +%Y%m%d-%H%M%S)"
    zfs snapshot "$src@_restore_tmp_${ts}"
    line="$(dataset_oldest_newest_snaps "$src")" || die "Unable to snapshot $src"
  fi

  local oldest newest
  oldest="$(awk '{print $1}' <<<"$line")"
  newest="$(awk '{print $2}' <<<"$line")"

  log "Replicating snapshots (history preserved):"
  log "  Oldest: $oldest"
  log "  Newest: $newest"

  # Perform full-history replication (-R includes descendants; -I oldest newest sends incrementals)
  # Use pv for visibility if possible.
  if [[ -n "$PV_BIN" ]]; then
    # We can't know send size easily; use pv in line mode for feedback.
    # zfs send outputs a stream; we just interpose pv for a visual indicator.
    zfs send -R -I "$oldest" "$newest" | "$PV_BIN" | zfs recv -F "$dst"
  else
    zfs send -R -I "$oldest" "$newest" | zfs recv -F "$dst"
  fi

  log "Replication complete: $dst"
}

# ------------------------------ Execution Flow ---------------------------------

log "Step 1/4: Ensuring temporary namespace exists…"
ensure_namespace

log "Step 2/4: Receiving FULL stream to namespace…"
stream_into_namespace "$FULL_ZST" "FULL"

if [[ -n "$INCR_ZST" ]]; then
  log "Step 3/4: Applying INCREMENTAL stream to namespace…"
  stream_into_namespace "$INCR_ZST" "INCREMENTAL"
else
  log "Step 3/4: No incremental provided — proceeding with FULL only."
fi

log "Enumerating top-level datasets under ${RESTORE_NS} (excluding ROOT)…"
children="$(list_toplevel_children)"
if [[ -z "$children" ]]; then
  die "No top-level datasets found under ${RESTORE_NS} to replicate. Check your streams."
fi

log "The following datasets will be replicated back into '${POOL_NAME}':"
printf "  - %s\n" $children

if ! confirm "Proceed with replication? This will overwrite existing targets with 'zfs recv -F'. [y/N]"; then
  die "User aborted before replication."
fi

log "Step 4/4: Replicating datasets (preserving snapshot history)…"
for ds in $children; do
  # Safety check: never touch ROOT
  [[ "$ds" == "${RESTORE_NS}/ROOT"* ]] && continue
  replicate_dataset_to_rpool "$ds"
done

log "All datasets replicated. Your fresh '${POOL_NAME}/ROOT' was not touched."
log "Next steps:"
printf -- "  • In Proxmox UI → Datacenter → Storage → ensure 'local-zfs' (or your ZFS storage) points to '%s'.\n" "$POOL_NAME"
echo   "  • Recreate or restore VM configs under /etc/pve/qemu-server/*.conf to reference disks like:"
echo   "      scsi0: local-zfs:vm-101-disk-0,size=XXG"
echo   "  • Start VMs with: qm start <VMID>"
log "Done ✅"
