#!/usr/bin/env bash
# ==============================================================================
#  zfs-r2 — Proxmox rpool FULL backup → Cloudflare R2
# ------------------------------------------------------------------------------
#  Design (crash-consistent, no Proxmox VM snapshots):
#    1) Single pool-wide snapshot: rpool@r2sync-<YYYYmmdd-HHMMSS>
#    2) Stream with `zfs send -R` → compress to single .zst in state/dump
#    3) Destroy snapshot **immediately after local dump completes**
#    4) Verify by full decompression locally
#    5) Upload to Cloudflare R2 atomically: .partial → moveto final
#    6) Healthchecks: /start on begin; post logs on success or /fail
#
#  Commands:
#    install      Initialize/OVERWRITE config & state; prompts for missing env
#    run          Execute backup now (default)
#    debug        Verbose run; `CONFIRM_COMMAND=1` to confirm each command
#    uninstall    Remove XDG config/state/cache
#    self-update  Update from UPDATE_URL (must be set)
#    recover      Recovery shell (15-min inactivity auto-exit)
#    help         Show help and environment reference
#
#  Key paths (root):
#    Config JSON : /etc/zfs-r2/config.json[.enc]
#    State dir   : /var/lib/zfs-r2
#    Cache dir   : /var/cache/zfs-r2
#    Dump dir    : /var/lib/zfs-r2/dump
#    Logs dir    : /var/lib/zfs-r2/logs (current.log, rotated)
#    rclone.conf : /etc/zfs-r2/rclone.conf
#
#  Healthchecks default:
#    https://healthchecks.megabyte.space/ping/csjKSM11DRvU5ZjHMmYxYg/zfs-r2
# ==============================================================================

# ──────────────────────────────────────────────────────────────────────────────
# Shell Expert library bootstrap (source.sh)
# ──────────────────────────────────────────────────────────────────────────────
U="${SE_LIB_URL:-https://public.megabyte.space/source.sh}"
P="${SE_LIB_PATH:-}"
if [[ -n "$P" && -r "$P" ]]; then . "$P"; else
  C=(/usr/local/lib/shell-expert/source.sh ./source.sh "${XDG_CACHE_HOME:-$HOME/.cache}/shell-expert/source.sh")
  for p in "${C[@]}"; do [[ -r "$p" ]] && { . "$p"; P="$p"; break; }; done
  if [[ -z "$P" ]]; then
    D="${XDG_CACHE_HOME:-$HOME/.cache}/shell-expert"; mkdir -p "$D" || { printf 'ERR: cache\n' >&2; exit 74; }
    T="$(mktemp "$D/.dl.XXXX")" || { printf 'ERR: mktemp\n' >&2; exit 70; }
    if command -v curl >/dev/null 2>&1; then curl -fsS --connect-timeout 5 -m 20 --retry 3 --retry-all-errors -o "$T" "$U" || { printf 'ERR: fetch\n' >&2; exit 69; }
    elif command -v wget >/dev/null 2>&1; then wget -q -O "$T" "$U" || { printf 'ERR: fetch\n' >&2; exit 69; }
    else printf 'ERR: need curl/wget\n' >&2; exit 69; fi
    bash -n "$T" || { printf 'ERR: syntax\n' >&2; exit 70; }
    P="$D/source.sh"; mv -f "$T" "$P"; chmod 0644 "$P" || true
    [[ -d /usr/local/lib && -w /usr/local/lib ]] && { mkdir -p /usr/local/lib/shell-expert 2>/dev/null || true; cp -f "$P" /usr/local/lib/shell-expert/source.sh 2>/dev/null || true; }
    . "$P"
  fi
fi

# ──────────────────────────────────────────────────────────────────────────────
# Constants / Defaults
# ──────────────────────────────────────────────────────────────────────────────
VERSION="${VERSION:-$(se_ts_ny_compact)}"
SLUG="zfs-r2"
UPDATE_URL="${UPDATE_URL:-}" # leave empty to skip self-update checks
DEFAULT_HC_URL="https://healthchecks.megabyte.space/ping/csjKSM11DRvU5ZjHMmYxYg/zfs-r2"

HOSTNAME_SHORT="$(hostname -s 2>/dev/null || printf 'host')"
DATASET="${DATASET:-rpool}"               # pool or top-level dataset to send
RCLONE_REMOTE_NAME="${RCLONE_REMOTE_NAME:-$SLUG}"
RCLONE_REMOTE_NAME="${RCLONE_REMOTE_NAME%%.*}"
RCLONE_REMOTE_NAME="${RCLONE_REMOTE_NAME//[^A-Za-z0-9_-]/-}"

# Globals for traps
SNAP_NAME=""           # set once snapshot is created
HC_URL=""              # cached healthchecks URL for this run
HC_SENT_START=0
HC_SENT_SUCCESS=0
HC_SENT_FAIL=0
ABORT_REASON=""

# ──────────────────────────────────────────────────────────────────────────────
# Initialize library context & update metadata
# ──────────────────────────────────────────────────────────────────────────────
se_init_context "$SLUG" "${UPDATE_URL:-local}"
se_set_update_url "$SLUG" "${UPDATE_URL:-}"
if [[ -n "$UPDATE_URL" ]]; then
  se_self_update "$SLUG" "$UPDATE_URL" "/usr/local/bin/$SLUG" || true
fi

# ──────────────────────────────────────────────────────────────────────────────
# Helpers: paths, JSON helpers, R2 naming, compression verify
# ──────────────────────────────────────────────────────────────────────────────
_cfg_path()    { se_xdg_config_file "$SLUG" "config.json"; }
_state_dir()   { se_xdg_state_dir  "$SLUG"; }
_cache_dir()   { se_xdg_cache_dir  "$SLUG"; }
_logs_dir()    { printf '%s/logs' "$(_state_dir)"; }
_dump_dir()    { printf '%s/dump' "$(_state_dir)"; }
_rclone_conf() { printf '%s/rclone.conf' "$(se_xdg_config_dir "$SLUG")"; }

_dataset_key()   { printf '%s' "${1//\//__}"; }
_full_path_for() { printf '%s/%s.full.zst' "$(_dump_dir)" "$(_dataset_key "$1")"; }

_numfmt() { command -v numfmt >/dev/null 2>&1 && numfmt "$@" || cat; }

se_json_get() { command -v jq >/dev/null 2>&1 || { printf ''; return 0; }; printf '%s' "$1" | jq -r "$2 // empty"; }

_object_key_full() {
  local dataset="$1" dkey; dkey="$(_dataset_key "$dataset")"
  local cfg rpfx; cfg="$(se_config_read "$SLUG")"; rpfx="$(se_json_get "$cfg" ".R2_PREFIX")"
  local prefix=""; [[ -n "$rpfx" ]] && prefix="${rpfx}/"
  printf '%shost=%s/dataset=%s/%s.full.zst' "$prefix" "$HOSTNAME_SHORT" "$dkey" "$dkey"
}

_est_send_bytes() { zfs send -nP -R "$1" 2>/dev/null | awk '/^size/ {print $2; exit}'; }

_verify_zst() {
  local f="$1" size=
  if command -v stat >/dev/null 2>&1; then
    size="$(stat -c '%s' -- "$f" 2>/dev/null || stat -f '%z' -- "$f" 2>/dev/null || true)"
  fi
  if command -v pv >/dev/null 2>&1; then
    if [[ -n "$size" ]]; then pv -s "$size" --name "verify:$(basename "$f")" < "$f" | zstd -dc -- >/dev/null
    else pv --name "verify:$(basename "$f")" < "$f" | zstd -dc -- >/dev/null; fi
  else
    dd if="$f" bs=4M status=none 2>/dev/null | zstd -dc -- >/dev/null
  fi
}

# ──────────────────────────────────────────────────────────────────────────────
# Snapshot cleanup & signal handling
# ──────────────────────────────────────────────────────────────────────────────
_cleanup_snapshot() {
  [[ -z "${SNAP_NAME:-}" ]] && return 0
  if zfs list -H -t snapshot -o name "${DATASET}@${SNAP_NAME}" >/dev/null 2>&1; then
    se_log_info "Cleanup trap: destroying ${DATASET}@${SNAP_NAME}"
    zfs destroy -r "${DATASET}@${SNAP_NAME}" >/dev/null 2>&1 || true
  fi
  SNAP_NAME=""
}

_on_sigint() { ABORT_REASON="SIGINT (Ctrl-C)"; exit 130; }
_on_sigterm(){ ABORT_REASON="SIGTERM";        exit 143; }

_on_exit() {
  # Ensure snapshot is gone
  _cleanup_snapshot

  # Post Healthchecks fail with log if run aborted or a step signaled failure.
  # The top-level runners also call se_hc_fail on step errors; guard to avoid double posts.
  local st=$?
  if [[ -n "$ABORT_REASON" && $HC_SENT_FAIL -eq 0 ]]; then
    se_log_warn "Run aborted: ${ABORT_REASON} (exit $st); sending /fail"
    se_hc_fail "${HC_URL:-$(se__hc_build_url "$SLUG")}"
    HC_SENT_FAIL=1
  fi
}
trap _on_exit EXIT
trap _on_sigint INT
trap _on_sigterm TERM

# ──────────────────────────────────────────────────────────────────────────────
# Install steps
# ──────────────────────────────────────────────────────────────────────────────
step_10_install_deps() { se_ensure_cmds zfs rclone zstd curl pv awk jq || return 69; }

step_20_install_dirs() { se_mkdir_p "$(_state_dir)" "$(_cache_dir)" "$(_dump_dir)" "$(_logs_dir)" || return 74; }

step_30_install_config() {
  # Defaults (read env if provided)
  local d_bucket d_prefix d_epdef d_ep d_ak d_sk d_sched d_bw d_chunk d_zlvl d_scrub d_hc
  d_bucket="${R2_BUCKET:-zfs}"
  d_prefix="${R2_PREFIX:-}"
  d_epdef="${R2_ENDPOINT_DEFAULT:-https://84fa0d1b16ff8086dd958c468ce7fd59.r2.cloudflarestorage.com}"
  d_ep="${R2_ENDPOINT:-}"
  d_ak="${R2_ACCESS_KEY_ID:-}"
  d_sk="${R2_SECRET_ACCESS_KEY:-}"
  d_sched="${BACKUP_SCHEDULE:-03:15}"
  d_bw="${RCLONE_BWLIMIT:-100M}"
  d_chunk="${RCLONE_CHUNK_SIZE:-128M}"
  d_zlvl="${ZSTD_LEVEL:-19}"
  d_scrub="${RUN_PARANOID_SCRUB:-0}"
  d_hc="${HEALTHCHECKS_URL:-$DEFAULT_HC_URL}"

  # Interactive prompts only when attached to a TTY and env not provided
  if [[ -t 0 && -z "${R2_BUCKET:-}" ]]; then read -r -p "R2 bucket name [$d_bucket]: " x; d_bucket="${x:-$d_bucket}"; fi
  if [[ -t 0 && -z "${R2_PREFIX:-}" ]]; then read -r -p "R2 key prefix (empty for none) [$d_prefix]: " x; d_prefix="${x:-$d_prefix}"; fi
  if [[ -t 0 && -z "${R2_ENDPOINT_DEFAULT:-}" ]]; then read -r -p "Cloudflare R2 endpoint (default) [$d_epdef]: " x; d_epdef="${x:-$d_epdef}"; fi
  if [[ -t 0 && -z "${R2_ENDPOINT:-}" ]]; then read -r -p "Override endpoint (empty to use default) [$d_ep]: " x; d_ep="${x:-$d_ep}"; fi
  if [[ -t 0 && -z "${R2_ACCESS_KEY_ID:-}" ]]; then read -r -p "R2 ACCESS KEY ID [$d_ak]: " x; d_ak="${x:-$d_ak}"; fi
  if [[ -t 0 && -z "${R2_SECRET_ACCESS_KEY:-}" ]]; then read -rs -p "R2 SECRET ACCESS KEY [hidden]: " x; printf '\n'; d_sk="${x:-$d_sk}"; fi
  if [[ -t 0 && -z "${BACKUP_SCHEDULE:-}" ]]; then read -r -p "Backup schedule (stored only) [$d_sched]: " x; d_sched="${x:-$d_sched}"; fi
  if [[ -t 0 && -z "${RCLONE_BWLIMIT:-}" ]]; then read -r -p "rclone bwlimit [$d_bw]: " x; d_bw="${x:-$d_bw}"; fi
  if [[ -t 0 && -z "${RCLONE_CHUNK_SIZE:-}" ]]; then read -r -p "rclone chunk_size [$d_chunk]: " x; d_chunk="${x:-$d_chunk}"; fi
  if [[ -t 0 && -z "${ZSTD_LEVEL:-}" ]]; then read -r -p "zstd level [$d_zlvl]: " x; d_zlvl="${x:-$d_zlvl}"; fi
  if [[ -t 0 && -z "${RUN_PARANOID_SCRUB:-}" ]]; then read -r -p "Run zpool scrub before backup? (0/1) [$d_scrub]: " x; d_scrub="${x:-$d_scrub}"; fi
  if [[ -t 0 && -z "${HEALTHCHECKS_URL:-}" ]]; then read -r -p "Healthchecks URL [$d_hc]: " x; d_hc="${x:-$d_hc}"; fi

  local cfg; cfg="$(jq -n \
    --arg bucket "$d_bucket" \
    --arg prefix "$d_prefix" \
    --arg epdef "$d_epdef" \
    --arg ep "$d_ep" \
    --arg ak "$d_ak" \
    --arg sk "$d_sk" \
    --arg sched "$d_sched" \
    --arg bw "$d_bw" \
    --arg chunk "$d_chunk" \
    --arg zlvl "$d_zlvl" \
    --arg scrub "$d_scrub" \
    --arg hc "$d_hc" \
    '{R2_BUCKET:$bucket,R2_PREFIX:$prefix,R2_ENDPOINT_DEFAULT:$epdef,R2_ENDPOINT:$ep,
      R2_ACCESS_KEY_ID:$ak,R2_SECRET_ACCESS_KEY:$sk,BACKUP_SCHEDULE:$sched,
      RCLONE_BWLIMIT:$bw,RCLONE_CHUNK_SIZE:$chunk,ZSTD_LEVEL:$zlvl,
      RUN_PARANOID_SCRUB:($scrub|tonumber),HEALTHCHECKS_URL:$hc,DATASET:"rpool"}' )"
  printf '%s\n' "$cfg" | se_config_write "$SLUG" || return 78
  se_log_info "Wrote config: $(_cfg_path)"
}

_write_rclone_remote() {
  local cfg="${1:-$(se_config_read "$SLUG")}"
  local endpoint; endpoint="$(se_json_get "$cfg" ".R2_ENDPOINT")"
  [[ -z "$endpoint" ]] && endpoint="$(se_json_get "$cfg" ".R2_ENDPOINT_DEFAULT")"
  local ak sk; ak="$(se_json_get "$cfg" ".R2_ACCESS_KEY_ID")"; sk="$(se_json_get "$cfg" ".R2_SECRET_ACCESS_KEY")"
  local rcconf; rcconf="$(_rclone_conf)"; se_mkdir_p "$(dirname "$rcconf")"
  : > "$rcconf" || return 1
  {
    printf '[%s]\n' "$RCLONE_REMOTE_NAME"
    printf 'type = s3\nprovider = Cloudflare\n'
    printf 'access_key_id = %s\n' "$ak"
    printf 'secret_access_key = %s\n' "$sk"
    printf 'endpoint = %s\n' "$endpoint"
    printf 'acl = private\nno_check_bucket = true\n'
    printf 'chunk_size = %s\n' "$(se_json_get "$cfg" ".RCLONE_CHUNK_SIZE")"
  } >>"$rcconf"
  chmod 600 "$rcconf" || true
  se_log_info "Configured rclone remote [$RCLONE_REMOTE_NAME] -> $endpoint"
}

step_40_install_rclone_remote() { _write_rclone_remote; }

_r2_write_test() {
  local cfg; cfg="$(se_config_read "$SLUG")"
  local ak sk; ak="$(se_json_get "$cfg" ".R2_ACCESS_KEY_ID")"; sk="$(se_json_get "$cfg" ".R2_SECRET_ACCESS_KEY")"
  if [[ -z "$ak" || -z "$sk" ]]; then
    se_log_warn "Skipping R2 write test (missing R2_ACCESS_KEY_ID/SECRET)."
    return 0
  fi
  local bucket rpfx; bucket="$(se_json_get "$cfg" ".R2_BUCKET")"; rpfx="$(se_json_get "$cfg" ".R2_PREFIX")"
  local key="${rpfx:+${rpfx}/}host=${HOSTNAME_SHORT}/.zfs-r2-write-test-${VERSION}"
  se_cmd rclone --config "$(_rclone_conf)" rcat "${RCLONE_REMOTE_NAME}:${bucket}/${key}" <<<"ok" || return 1
  se_cmd rclone --config "$(_rclone_conf)" deletefile "${RCLONE_REMOTE_NAME}:${bucket}/${key}" || true
}

step_50_install_test_r2() { _r2_write_test || se_log_warn "R2 write test failed — verify credentials/endpoint/bucket."; }

step_60_install_persist_update_url() { se_set_update_url "$SLUG" "${UPDATE_URL:-}"; }

# ──────────────────────────────────────────────────────────────────────────────
# Run steps
# ──────────────────────────────────────────────────────────────────────────────
step_10_preflight() {
  se_ensure_cmds zfs rclone zstd pv awk jq curl || return 69
  local cfg; cfg="$(se_config_read "$SLUG")"
  [[ -n "$(se_json_get "$cfg" ".R2_BUCKET")" ]] || { se_log_error "Missing R2 config (run install)"; return 78; }
  zfs list -H -o name "$DATASET" >/dev/null 2>&1 || { se_log_error "Dataset not found: $DATASET"; return 66; }
  _write_rclone_remote "$cfg" || return 70
  return 0
}

step_15_prepare_dirs() { se_mkdir_p "$(_dump_dir)" "$(_logs_dir)" || return 74; }

step_20_optional_scrub() {
  local cfg; cfg="$(se_config_read "$SLUG")"
  if [[ "$(se_json_get "$cfg" ".RUN_PARANOID_SCRUB")" == "1" ]]; then
    se_cmd zpool scrub "$DATASET" || se_log_warn "zpool scrub failed (continuing)"
  fi
}

step_30_create_snapshot() {
  SNAP_NAME="r2sync-${VERSION}"
  se_log_info "Creating atomic pool-wide snapshot: ${DATASET}@${SNAP_NAME}"
  se_cmd zfs snapshot -r "${DATASET}@${SNAP_NAME}" || return 70
}

step_40_full_send_to_dump() {
  local cfg zlvl out_final out_tmp est
  cfg="$(se_config_read "$SLUG")"
  zlvl="$(se_json_get "$cfg" ".ZSTD_LEVEL")"
  out_final="$(_full_path_for "$DATASET")"
  out_tmp="${out_final}.partial"
  se_mkdir_p "$(dirname "$out_final")" || return 74

  est="$(_est_send_bytes "${DATASET}@${SNAP_NAME}")"
  local est_human; est_human="$(_numfmt --to=iec --suffix=B "${est:-0}" 2>/dev/null || printf '%s' "${est:-0}")"
  se_log_info "Creating FULL ${DATASET}@${SNAP_NAME} → ${out_final} (est: ${est_human:-unknown})"

  set -o pipefail
  if [[ -n "$est" ]]; then
    zfs send -R "${DATASET}@${SNAP_NAME}" | { command -v pv >/dev/null 2>&1 && pv -s "$est" || cat; } | zstd -"${zlvl}" -T0 > "$out_tmp"
  else
    zfs send -R "${DATASET}@${SNAP_NAME}" | { command -v pv >/dev/null 2>&1 && pv || cat; } | zstd -"${zlvl}" -T0 > "$out_tmp"
  fi
  local rc=$?; set +o pipefail; (( rc == 0 )) || return 70

  sync; mv -f "$out_tmp" "$out_final" || return 74
  se_log_info "FULL written: $out_final"
}

# Destroy the snapshot ASAP, *before* verify/upload
step_50_destroy_snapshot_early() {
  if [[ -n "${SNAP_NAME:-}" ]]; then
    se_log_info "Destroying ephemeral snapshot ASAP: ${DATASET}@${SNAP_NAME}"
    se_cmd zfs destroy -r "${DATASET}@${SNAP_NAME}" || se_log_warn "Failed to destroy ${DATASET}@${SNAP_NAME} (continuing)"
    SNAP_NAME=""
  fi
}

step_60_verify_local() {
  local f; f="$(_full_path_for "$DATASET")"
  se_log_info "Verifying local cache: $f"
  _verify_zst "$f" || return 70
}

step_70_upload_to_r2() {
  local cfg remote bucket bw f key
  cfg="$(se_config_read "$SLUG")"
  remote="$RCLONE_REMOTE_NAME"
  bucket="$(se_json_get "$cfg" ".R2_BUCKET")"
  bw="$(se_json_get "$cfg" ".RCLONE_BWLIMIT")"
  f="$(_full_path_for "$DATASET")"
  [[ -f "$f" ]] || { se_log_error "Missing dump file: $f"; return 66; }
  key="$(_object_key_full "$DATASET")"

  se_log_info "Uploading $(basename "$f") → ${key}"
  if ! se_cmd rclone --config "$(_rclone_conf)" copyto "$f" "${remote}:${bucket}/${key}.partial" ${bw:+--bwlimit "$bw"} --stats=10s; then
    return 69
  fi
  se_cmd rclone --config "$(_rclone_conf)" moveto "${remote}:${bucket}/${key}.partial" "${remote}:${bucket}/${key}" || true
}

# Healthchecks success step (explicit so it's part of ordered steps)
step_80_healthchecks_success() {
  se_hc_success "$HC_URL"
  HC_SENT_SUCCESS=1
}

# ──────────────────────────────────────────────────────────────────────────────
# Runners
# ──────────────────────────────────────────────────────────────────────────────
_install() {
  HC_URL="$(se__hc_build_url "$SLUG")"
  se_hc_start "$HC_URL"; HC_SENT_START=1

  se_ndjson_init "$SLUG"
  if ! se_run_steps \
      step_10_install_deps \
      step_20_install_dirs \
      step_30_install_config \
      step_40_install_rclone_remote \
      step_50_install_test_r2 \
      step_60_install_persist_update_url
  then
    local rc=$?
    se_ndjson_finalize "$VERSION" "$rc" "${START_FROM:-}" "" "fail"
    se_hc_fail "$HC_URL"; HC_SENT_FAIL=1
    return "$rc"
  fi
  se_ndjson_finalize "$VERSION" 0 "${START_FROM:-}" "" "ok"
  se_hc_success "$HC_URL"; HC_SENT_SUCCESS=1
  return 0
}

_run() {
  HC_URL="$(se__hc_build_url "$SLUG")"
  se_hc_start "$HC_URL"; HC_SENT_START=1

  se_ndjson_init "$SLUG"
  if ! se_run_steps \
      step_10_preflight \
      step_15_prepare_dirs \
      step_20_optional_scrub \
      step_30_create_snapshot \
      step_40_full_send_to_dump \
      step_50_destroy_snapshot_early \
      step_60_verify_local \
      step_70_upload_to_r2 \
      step_80_healthchecks_success
  then
    local rc=$?
    se_ndjson_finalize "$VERSION" "$rc" "${START_FROM:-}" "" "fail"
    if [[ $HC_SENT_FAIL -eq 0 ]]; then se_hc_fail "$HC_URL"; HC_SENT_FAIL=1; fi
    return "$rc"
  fi
  se_ndjson_finalize "$VERSION" 0 "${START_FROM:-}" "" "ok"
  return 0
}

_help() {
  cat <<'EOF'
zfs-r2 — Proxmox rpool FULL backup → Cloudflare R2

Commands
  install      Initialize/OVERWRITE config & state; prompts for missing env
  run          Execute backup now (default)
  debug        Verbose run; set CONFIRM_COMMAND=1 to confirm each command
  uninstall    Remove XDG config/state/cache
  self-update  Update from UPDATE_URL (must be set)
  recover      Recovery shell (15-min inactivity auto-exit)
  help         Show this help

Snapshot behavior
  • Snapshot is destroyed immediately after the local dump is completed
    (BEFORE verification or upload) so Proxmox can fully control snapshots.
  • A cleanup trap also removes the snapshot on failure or Ctrl-C.

Environment (read at install; persisted to config.json[.enc])
+----------------------+---------------------------------------------+-----------+--------------------------------------------+
| R2_BUCKET            | zfs                                         | Rec       | R2 bucket name                              |
| R2_PREFIX            | ""                                          | Opt       | Prefix within bucket                        |
| R2_ENDPOINT_DEFAULT  | Cloudflare R2 URL                           | Provided  | Default endpoint                             |
| R2_ENDPOINT          | ""                                          | Opt       | Custom endpoint                              |
| R2_ACCESS_KEY_ID     | ""                                          | Rec       | Access key                                   |
| R2_SECRET_ACCESS_KEY | ""                                          | Rec       | Secret key                                   |
| BACKUP_SCHEDULE      | 03:15                                       | Opt       | Stored only (no scheduler managed)           |
| RCLONE_BWLIMIT       | 100M                                        | Opt       | rclone bwlimit                                |
| RCLONE_CHUNK_SIZE    | 128M                                        | Opt       | rclone chunk size                             |
| ZSTD_LEVEL           | 19                                          | Opt       | zstd compression level                        |
| RUN_PARANOID_SCRUB   | 0                                           | Opt       | 1 = run zpool scrub before backup            |
| HEALTHCHECKS_URL     | https://healthchecks.megabyte.space/ping/...| Opt/Def   | Healthchecks endpoint for this job           |
| UPDATE_URL           | ""                                          | Opt       | Self-update URL for this script               |
+----------------------+---------------------------------------------+-----------+--------------------------------------------+

Examples
  sudo zfs-r2 install
  sudo R2_BUCKET=my-bkt R2_PREFIX=$(hostname -s) zfs-r2 install
  sudo zfs-r2 run
  sudo CONFIRM_COMMAND=1 zfs-r2 debug
  sudo UPDATE_URL="https://example.org/zfs-r2" zfs-r2 self-update

EOF
}

# ──────────────────────────────────────────────────────────────────────────────
# CLI
# ──────────────────────────────────────────────────────────────────────────────
main() {
  case "${1:-run}" in
    install)     shift; _install "$@";;
    run)         shift; _run "$@";;
    debug)       DEBUG=1 CONFIRM_COMMAND=${CONFIRM_COMMAND:-1} shift; _run "$@";;
    uninstall)   rm -rf "$(se_xdg_config_dir "$SLUG")" "$(se_xdg_state_dir "$SLUG")" "$(se_xdg_cache_dir "$SLUG")";;
    self-update) [[ -n "$UPDATE_URL" ]] && se_self_update "$SLUG" "$UPDATE_URL" "/usr/local/bin/$SLUG" || se_log_warn "UPDATE_URL is empty; skipping.";;
    recover)     se_recover_shell "$SLUG";;
    help|-h|--help) _help;;
    *)           printf 'Unknown command: %s\n' "$1" >&2; _help; exit 64;;
  esac
}
main "$@"
